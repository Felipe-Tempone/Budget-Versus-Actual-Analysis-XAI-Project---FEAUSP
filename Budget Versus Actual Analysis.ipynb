{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Constants\n",
    "OUTPUT_DIR = \"output\"\n",
    "EXCEL_RESULTS_FILE = os.path.join(OUTPUT_DIR, \"fpna_results.xlsx\")\n",
    "SHAP_PLOT_FILE = os.path.join(OUTPUT_DIR, \"shap_summary.png\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(OUTPUT_DIR, \"fpna_analysis.log\")),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def ensure_output_dir():\n",
    "    \"\"\"\n",
    "    Ensure the output directory exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        logging.info(f\"Created output directory at {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        logging.info(f\"Output directory exists at {OUTPUT_DIR}\")\n",
    "\n",
    "def load_dummy_data(seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate dummy FP&A data for testing.  \n",
    "    Args:\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        pd.DataFrame: Generated dummy data.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = 100  # number of records\n",
    "    \n",
    "    data = {\n",
    "        \"Country\": np.random.choice([\"USA\", \"Germany\", \"Brazil\", \"India\"], size=n),\n",
    "        \"Department\": np.random.choice([\"Sales\", \"R&D\", \"Finance\", \"Operations\"], size=n),\n",
    "        \"Account\": np.random.choice([\"Revenue\", \"Cost\", \"Profit\", \"Investment\"], size=n),\n",
    "        \"Budget\": np.random.uniform(10000, 50000, size=n),\n",
    "        \"Actual\": np.random.uniform(9000, 55000, size=n),\n",
    "        \"Month\": np.random.choice(pd.date_range(\"2024-01-01\", periods=12, freq='M')_\n",
    "                                  .strftime('%Y-%m'), size=n),\n",
    "        \"Historical_Variance\": np.random.normal(0, 0.05, size=n),  # +/- 5% variance\n",
    "        \"Market_Index\": np.random.uniform(0.8, 1.2, size=n),\n",
    "        \"Operational_Efficiency\": np.random.uniform(0.7, 1.1, size=n)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    logging.info(f\"Dummy data loaded with shape {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_rule_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate rule-based anomaly scores for each record.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input FP&A data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added anomaly score columns.\n",
    "    \"\"\"\n",
    "    logging.info(\"Calculating rule-based anomaly scores.\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    df['Budget'] = df['Budget'].replace(0, np.nan)\n",
    "    \n",
    "    # Rule 1: Variance percentage\n",
    "    df['Variance_Pct'] = (df['Actual'] - df['Budget']) / df['Budget']\n",
    "    \n",
    "    # Rule 2: Historical variance deviation\n",
    "    df['Historical_Adj'] = df['Variance_Pct'] - df['Historical_Variance']\n",
    "    \n",
    "    # Rule 3: Market impact adjustment\n",
    "    df['Market_Adj'] = df['Variance_Pct'] / df['Market_Index']\n",
    "    \n",
    "    # Rule 4: Operational efficiency adjustment\n",
    "    df['OpEff_Adj'] = df['Variance_Pct'] / df['Operational_Efficiency']\n",
    "    \n",
    "    # Combine rules into a composite anomaly score (weighted sum)\n",
    "    weights = {\n",
    "        'Variance_Pct': 0.4,\n",
    "        'Historical_Adj': 0.2,\n",
    "        'Market_Adj': 0.2,\n",
    "        'OpEff_Adj': 0.2\n",
    "    }\n",
    "    \n",
    "    df['Anomaly_Score'] = (\n",
    "        weights['Variance_Pct'] * df['Variance_Pct'].abs() +\n",
    "        weights['Historical_Adj'] * df['Historical_Adj'].abs() +\n",
    "        weights['Market_Adj'] * df['Market_Adj'].abs() +\n",
    "        weights['OpEff_Adj'] * df['OpEff_Adj'].abs()\n",
    "    )\n",
    "    \n",
    "    # Normalize anomaly score between 0 and 1\n",
    "    max_score = df['Anomaly_Score'].max()\n",
    "    if max_score > 0:\n",
    "        df['Anomaly_Score'] = df['Anomaly_Score'] / max_score\n",
    "    else:\n",
    "        df['Anomaly_Score'] = 0\n",
    "    \n",
    "    logging.info(\"Rule-based anomaly scores calculated.\")\n",
    "    return df\n",
    "\n",
    "def parallel_rule_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parallelize rule score calculation for large datasets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input FP&A data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with anomaly scores.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting parallel anomaly score calculation.\")\n",
    "    n_jobs = min(4, os.cpu_count() or 1)\n",
    "    splits = np.array_split(df, n_jobs)\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(calculate_rule_scores)(split) for split in splits\n",
    "    )\n",
    "    \n",
    "    combined_df = pd.concat(results, axis=0).reset_index(drop=True)\n",
    "    logging.info(\"Parallel anomaly score calculation completed.\")\n",
    "    return combined_df\n",
    "\n",
    "def explain_with_shap(df: pd.DataFrame, feature_cols: \n",
    "                      list, target_col: str = \"Anomaly_Score\")\n",
    "                        -> None:\n",
    "    \"\"\"\n",
    "    Generate SHAP explanations and save summary plot.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing features and target.\n",
    "        feature_cols (list): List of feature column names.\n",
    "        target_col (str): Name of the target column for explanation.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting SHAP explainability analysis.\")\n",
    "    try:\n",
    "        X = df[feature_cols]\n",
    "        y = df[target_col]\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.summary_plot(shap_values, X, feature_names=feature_cols, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(SHAP_PLOT_FILE, dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        logging.info(f\"SHAP summary plot saved to {SHAP_PLOT_FILE}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SHAP explanation failed: {e}\", exc_info=True)\n",
    "\n",
    "def save_to_excel(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save DataFrame to Excel with conditional formatting and summary.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save.\n",
    "        filename (str): Output Excel file path.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Saving results to Excel file {filename}\")\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name='Results')\n",
    "            \n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets['Results']\n",
    "            \n",
    "            # Apply conditional formatting on Anomaly_Score column (assumed column J)\n",
    "            anomaly_col_idx = df.columns.get_loc('Anomaly_Score')\n",
    "            col_letter = chr(ord('A') + anomaly_col_idx)\n",
    "            data_range = f\"{col_letter}2:{col_letter}{len(df)+1}\"\n",
    "            \n",
    "            worksheet.conditional_format(data_range, {\n",
    "                'type': '3_color_scale',\n",
    "                'min_color': \"#63BE7B\",\n",
    "                'mid_color': \"#FFEB84\",\n",
    "                'max_color': \"#F8696B\"\n",
    "            })\n",
    "            \n",
    "            # Write summary statistics in a new sheet\n",
    "            summary = df.describe()\n",
    "            summary.to_excel(writer, sheet_name='Summary')\n",
    "            \n",
    "        logging.info(f\"Excel file saved successfully: {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save Excel file: {e}\", exc_info=True)\n",
    "\n",
    "def run_analysis() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main analysis workflow: load data, calculate anomaly scores,\n",
    "    explain, and save results.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final DataFrame with anomaly scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ensure_output_dir()\n",
    "        \n",
    "        # Load or generate FP&A data\n",
    "        df = load_dummy_data()\n",
    "        \n",
    "        # Calculate anomaly scores (parallelized)\n",
    "        df_scored = parallel_rule_scores(df)\n",
    "        \n",
    "        # Features used in scoring for SHAP explanation\n",
    "        feature_cols = ['Variance_Pct', 'Historical_Adj', 'Market_Adj', 'OpEff_Adj']\n",
    "        \n",
    "        # Generate SHAP explanations\n",
    "        explain_with_shap(df_scored, feature_cols)\n",
    "        \n",
    "        # Save results to Excel\n",
    "        save_to_excel(df_scored, EXCEL_RESULTS_FILE)\n",
    "        \n",
    "        logging.info(\"FP&A analysis workflow completed successfully.\")\n",
    "        return df_scored\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FP&A analysis workflow failed: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Command-line entry point for the FP&A explainable AI analysis.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting FP&A Explainable AI analysis.\")\n",
    "    result_df = run_analysis()\n",
    "    if result_df is not None:\n",
    "        print(\"\\nTop 10 anomalies detected:\")\n",
    "        print(result_df[['Country', 'Department', 'Account', 'Month', 'Anomaly_Score']]_\n",
    "              .sort_values(\n",
    "            by='Anomaly_Score', ascending=False).head(10))\n",
    "    else:\n",
    "        print(\"Analysis failed. Please check the logs for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
